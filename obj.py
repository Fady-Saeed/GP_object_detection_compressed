#!/usr/bin/env python


import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    open(os.path.join(partial_path, "__init__.py"), "w").write("\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "w") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('detection_utils.py', 'from keras.models import load_model\nimport cv2\nimport numpy as np\nfrom model_utils import yolo_eval\nfrom keras import backend as K\nimport tensorflow as tf\nimport colorsys\nfrom timeit import default_timer as timer\nfrom sort import *\nimport lane\n\ndefaults = {\n        "model_path": \'./model.h5\',\n        "anchors": np.array([5,7,  11,13,  18,29,  40,41,  119,148,  289,253]).reshape(-1, 2),\n        "classes": ["bicycle","car","motorbike","person","cones"],\n        "score" : 0.45,\n        "iou" : 0.45,\n        "model_image_size" : (384, 384),\n        "gpu_num" : 1,\n        "frames_processed_every" : 3\n    }\n\n\n\ndef init(model):\n     global input_image_shape \n     input_image_shape = K.placeholder(shape=(2, ))\n     global _model\n     _model = model\n\n     global boxes, scores, classes \n     boxes, scores, classes = yolo_eval(model.output,defaults["anchors"],len(defaults["classes"]),input_image_shape,\n        score_threshold=defaults["score"],iou_threshold=defaults["iou"])\n\n     global sess \n     sess = K.get_session()\n\ndef finalize():\n    sess.close()\n\n\ndef intermediate_models(image, lane_model):\n    if lane_model == None: \n        return image\n    l = lane.Lanes()\n    image = lane.draw_lane(lane_model, l, image)\n    return image\n\ndef letterbox_image(image, size):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    ih, iw = image.shape[:2]\n    h, w = size\n    scale = min(w/iw, h/ih)\n    nw = int(iw*scale)\n    nh = int(ih*scale)\n\n    sizeRGB = list(size)\n    sizeRGB.append(3)\n    image = cv2.resize(image,(int(nw),int(nh)), interpolation=cv2.INTER_CUBIC)\n    new_image = np.full(tuple(sizeRGB), 128)\n    new_image[int((h-nh)//2) : int((h-nh)//2 + nh) ,int((w-nw)//2) : int((w-nw)//2) + nw] = image\n    return new_image\n\n\ndef get_drawing_materials(image):\n    class_names = defaults["classes"]\n    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(\n        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n    np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n    np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n    np.random.seed(None)  # Reset seed to default.\n\n    thickness = (image.shape[0] + image.shape[1]) // 350\n    return colors, thickness\n\ndef get_detection_results(image, model):\n    boxes_image = np.array([])\n    #resizing processed image\n    if defaults["model_image_size"] != (None, None):\n        assert defaults["model_image_size"][0]%32 == 0, \'Multiples of 32 required\'\n        assert defaults["model_image_size"][1]%32 == 0, \'Multiples of 32 required\'\n        boxes_image = letterbox_image(image, defaults["model_image_size"])\n    else:\n        new_image_size = (image.shape[0] - (image.shape[0] % 32),\n                          image.shape[1] - (image.shape[1] % 32))\n        boxed_image = letterbox_image(image, new_image_size)\n\n\n    image_data = np.array(boxes_image, dtype=\'float32\')\n\n    image_data /= 255.\n    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n\n\n    out_boxes, out_scores, out_classes = sess.run(\n                [boxes, scores, classes],\n                feed_dict={\n                    model.input: image_data,\n                    input_image_shape: [image.shape[0], image.shape[1]],\n                    K.learning_phase(): 0\n                })\n\n    return out_boxes, out_scores, out_classes\n\n\ndef detect_image(image):\n    out_boxes, out_scores, out_classes = get_detection_results(image, _model)\n    colors, thickness = get_drawing_materials(image)\n    return (out_boxes, out_scores, out_classes, colors, thickness)\n\n\ndef draw_on_image(image, processed_data):\n    out_boxes, out_scores, out_classes, colors, thickness, dist_model, out_ids = processed_data\n\n    for i, c in reversed(list(enumerate(out_classes))):\n            predicted_class = defaults["classes"][int(c)]\n            box = out_boxes[i]\n            score = out_scores[i]\n            top, left, bottom, right = box\n\n\n            ##construct tracking labels\n            if out_ids is not None:\n                id_label_part = " id = {}".format(out_ids[i])\n\n            else:\n                id_label_part = ""\n\n\n            ##construct distance labels\n            if dist_model != None:\n                z_dist = dist_model.predict(np.array([[left/image.shape[1],top/image.shape[0],right/image.shape[1],bottom/image.shape[0]]]))[0,0] * 0.5\n                \n                object_center_x, object_center_y = (left-right)/2, (top-bottom)/2\n                road_center_x, road_center_y = image.shape[0]/2, image.shape[1]/2 # AKA vanishing point for simplicity\n                x_dist = (object_center_x - road_center_x) / image.shape[0] * 0.5\n                y_dist = (object_center_y - road_center_y/2) / image.shape[1] * 0.5 # Assume that cars only appear in the lower half of the image\n                \n                dist = np.cbrt(z_dist**3 + x_dist**3 + y_dist**3)\n                dist_label_part = " distance = {:.2f}".format(dist/2)\n\n            else:\n                dist_label_part = ""\n                \n            ##construct full labels\n            label = \'{}\'.format(predicted_class) +  dist_label_part + id_label_part\n\n\n\n                \n            (label_width, label_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, thickness)\n            print(label, (left, top), (right, bottom))\n            top = min(image.shape[0], np.floor(top - 0.5).astype(\'int32\'))\n            left = min(image.shape[1], np.floor(left - 0.5).astype(\'int32\'))\n            bottom = max(0, np.floor(bottom + 0.5).astype(\'int32\'))\n            right = max(0, np.floor(right + 0.5).astype(\'int32\'))\n            if top <= image.shape[0] :\n                text_origin = np.array([left, top])\n            else:\n                text_origin = np.array([left, top - 1])\n\n            text_originPlusSize = [int((text_origin[0] + label_width*1.5)), int((text_origin[1] + label_height*1.5))]\n\n            text_originReal = [text_origin[0] + int(thickness*1.5), text_origin[1] + label_height]\n\n            image = cv2.rectangle(image, (left, top), (right, bottom), colors[int(c)], thickness)\n            image = cv2.rectangle(image, tuple(text_origin), tuple(text_originPlusSize), colors[int(c)], -1)\n            cv2.putText(image, text=label, org=tuple(text_originReal), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                                fontScale=0.5, color=(0, 0, 0), thickness=min(2,int(thickness/1.1)))\n    return image\n\n\ndef detect_video(video_path, freeze,  output_path="", dist_model=None, lane_model=None, tracking=False):\n    if video_path=="0":\n        vid = cv2.VideoCapture(0)\n    else:    \n        vid = cv2.VideoCapture(video_path)\n    if not vid.isOpened():\n        raise IOError("Couldn\'t open webcam or video")\n    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    isOutput = True if output_path != "" else False\n    if isOutput:\n        print("!!! TYPE:", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n    accum_time = 0\n    curr_fps = 0\n    fps = "FPS: ??"\n    prev_time = timer()\n    counter=0\n\n    #initialize tracking\n    if tracking:\n        tracker = Sort()\n    else:\n        tracker = None\n\n\n    while True:\n        #reading video frames\n        return_value, frame = vid.read()\n        if not return_value:\n            if isOutput:\n                out.release()\n            vid.release()\n            break\n        image = frame\n        if(counter%freeze == 0):\n            pred_out_boxes, out_scores, out_classes, colors, thickness = detect_image(image)\n        \n        counter += 1\n\n        #update bounding boxes by the ones from the tracker\n        if tracking:\n            tracked_boxes = tracker.update(np.column_stack((pred_out_boxes[:,1], pred_out_boxes[:,0], pred_out_boxes[:,3], pred_out_boxes[:,2])))\n            out_boxes =  np.column_stack((tracked_boxes[:,1], tracked_boxes[:,0], tracked_boxes[:,3], tracked_boxes[:,2]))\n\n            if out_boxes.shape[0] == pred_out_boxes.shape[0]:\n                out_ids = tracked_boxes[:,4]\n\n            else:\n                out_boxes = pred_out_boxes\n                out_ids = None\n\n        else:\n            out_boxes = pred_out_boxes\n            out_ids = None\n\n\n\n        ##put intermediate models code here\n        image = intermediate_models(image, lane_model)\n\n        image = draw_on_image(image, (out_boxes, out_scores, out_classes, colors, thickness, dist_model,out_ids))\n        result = np.asarray(image)\n        curr_time = timer()\n        exec_time = curr_time - prev_time\n        prev_time = curr_time\n        accum_time = accum_time + exec_time\n        curr_fps = curr_fps + 1\n        if accum_time > 1:\n            accum_time = accum_time - 1\n            fps = "FPS: " + str(curr_fps)\n            curr_fps = 0\n        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n        cv2.namedWindow("result", cv2.WINDOW_NORMAL)\n        cv2.imshow("result", result)\n        if isOutput:\n            out.write(result)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            if isOutput:\n                out.release()\n            vid.release()\n            break\n')
    __stickytape_write_module('model_utils.py', 'from keras import backend as K\nimport tensorflow as tf\nimport cv2\nimport numpy as np\n\ndef yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n    """Convert final layer features to bounding box parameters."""\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n\n    grid_shape = K.shape(feats)[1:3] # height, width\n    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n        [1, grid_shape[1], 1, 1])\n    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n        [grid_shape[0], 1, 1, 1])\n    grid = K.concatenate([grid_x, grid_y])\n    grid = K.cast(grid, K.dtype(feats))\n\n    feats = K.reshape(\n        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n    box_confidence = K.sigmoid(feats[..., 4:5])\n    box_class_probs = K.sigmoid(feats[..., 5:])\n\n    if calc_loss == True:\n        return grid, feats, box_xy, box_wh\n    return box_xy, box_wh, box_confidence, box_class_probs\n\n\ndef yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n    \'\'\'Process Conv layer output\'\'\'\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n        anchors, num_classes, input_shape)\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n    boxes = K.reshape(boxes, [-1, 4])\n    box_scores = box_confidence * box_class_probs\n    box_scores = K.reshape(box_scores, [-1, num_classes])\n    return boxes, box_scores\n\n\n\ndef yolo_eval(yolo_outputs,\n              anchors,\n              num_classes,\n              image_shape,\n              max_boxes=20,\n              score_threshold=.6,\n              iou_threshold=.5):\n    """Evaluate YOLO model on given input and return filtered boxes."""\n    num_layers = len(yolo_outputs)\n    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n    boxes = []\n    box_scores = []\n    for l in range(num_layers):\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n        boxes.append(_boxes)\n        box_scores.append(_box_scores)\n    boxes = K.concatenate(boxes, axis=0)\n    box_scores = K.concatenate(box_scores, axis=0)\n\n    mask = box_scores >= score_threshold\n    max_boxes_tensor = K.constant(max_boxes, dtype=\'int32\')\n    boxes_ = []\n    scores_ = []\n    classes_ = []\n    for c in range(num_classes):\n        # TODO: use keras backend instead of tf.\n        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n        nms_index = tf.image.non_max_suppression(\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n        class_boxes = K.gather(class_boxes, nms_index)\n        class_box_scores = K.gather(class_box_scores, nms_index)\n        classes = K.ones_like(class_box_scores, \'int32\') * c\n        boxes_.append(class_boxes)\n        scores_.append(class_box_scores)\n        classes_.append(classes)\n    boxes_ = K.concatenate(boxes_, axis=0)\n    scores_ = K.concatenate(scores_, axis=0)\n    classes_ = K.concatenate(classes_, axis=0)\n\n    return boxes_, scores_, classes_\n\n\ndef yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n    \'\'\'Get corrected boxes\'\'\'\n    box_yx = box_xy[..., ::-1]\n    box_hw = box_wh[..., ::-1]\n    input_shape = K.cast(input_shape, K.dtype(box_yx))\n    image_shape = K.cast(image_shape, K.dtype(box_yx))\n    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n    offset = (input_shape-new_shape)/2./input_shape\n    scale = input_shape/new_shape\n    box_yx = (box_yx - offset) * scale\n    box_hw *= scale\n\n    box_mins = box_yx - (box_hw / 2.)\n    box_maxes = box_yx + (box_hw / 2.)\n    boxes =  K.concatenate([\n        box_mins[..., 0:1],  # y_min\n        box_mins[..., 1:2],  # x_min\n        box_maxes[..., 0:1],  # y_max\n        box_maxes[..., 1:2]  # x_max\n    ])\n\n    # Scale boxes back to original image shape.\n    boxes *= K.concatenate([image_shape, image_shape])\n    return boxes\n\n')
    __stickytape_write_module('sort.py', 'from __future__ import print_function\n\nfrom numba import jit\nimport os.path\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom skimage import io\nfrom sklearn.utils.linear_assignment_ import linear_assignment\nimport time\nimport argparse\nfrom filterpy.kalman import KalmanFilter\n\n\n@jit\ndef iou(bb_test, bb_gt):\n  """\n  Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n  """\n  xx1 = np.maximum(bb_test[0], bb_gt[0])\n  yy1 = np.maximum(bb_test[1], bb_gt[1])\n  xx2 = np.minimum(bb_test[2], bb_gt[2])\n  yy2 = np.minimum(bb_test[3], bb_gt[3])\n  w = np.maximum(0., xx2 - xx1)\n  h = np.maximum(0., yy2 - yy1)\n  wh = w * h\n  o = wh / ((bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n            + (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh)\n  return o\n\n\ndef convert_bbox_to_z(bbox):\n  """\n  Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n    the aspect ratio\n  """\n  w = bbox[2] - bbox[0]\n  h = bbox[3] - bbox[1]\n  x = bbox[0] + w / 2.\n  y = bbox[1] + h / 2.\n  s = w * h  # scale is just area\n  r = w / float(h)\n  return np.array([x, y, s, r]).reshape((4, 1))\n\n\ndef convert_x_to_bbox(x, score=None):\n  """\n  Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n  """\n  w = np.sqrt(x[2] * x[3])\n  h = x[2] / w\n  if score is None:\n    return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2.]).reshape((1, 4))\n  else:\n    return np.array([x[0] - w / 2., x[1] - h / 2., x[0] + w / 2., x[1] + h / 2., score]).reshape((1, 5))\n\n\nclass KalmanBoxTracker(object):\n  """\n  This class represents the internel state of individual tracked objects observed as bbox.\n  """\n  count = 0\n\n  def __init__(self, bbox):\n    """\n    Initialises a tracker using initial bounding box.\n    """\n    # define constant velocity model\n    self.kf = KalmanFilter(dim_x=7, dim_z=4)\n    self.kf.F = np.array(\n      [[1, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]])\n    self.kf.H = np.array(\n      [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0]])\n\n    self.kf.R[2:, 2:] *= 10.\n    self.kf.P[4:, 4:] *= 1000.  # give high uncertainty to the unobservable initial velocities\n    self.kf.P *= 10.\n    self.kf.Q[-1, -1] *= 0.01\n    self.kf.Q[4:, 4:] *= 0.01\n\n    self.kf.x[:4] = convert_bbox_to_z(bbox)\n    self.time_since_update = 0\n    self.id = KalmanBoxTracker.count\n    KalmanBoxTracker.count += 1\n    self.history = []\n    self.hits = 0\n    self.hit_streak = 0\n    self.age = 0\n\n  def update(self, bbox):\n    """\n    Updates the state vector with observed bbox.\n    """\n    self.time_since_update = 0\n    self.history = []\n    self.hits += 1\n    self.hit_streak += 1\n    self.kf.update(convert_bbox_to_z(bbox))\n\n  def predict(self):\n    """\n    Advances the state vector and returns the predicted bounding box estimate.\n    """\n    if ((self.kf.x[6] + self.kf.x[2]) <= 0):\n      self.kf.x[6] *= 0.0\n    self.kf.predict()\n    self.age += 1\n    if (self.time_since_update > 0):\n      self.hit_streak = 0\n    self.time_since_update += 1\n    self.history.append(convert_x_to_bbox(self.kf.x))\n    return self.history[-1]\n\n  def get_state(self):\n    """\n    Returns the current bounding box estimate.\n    """\n    return convert_x_to_bbox(self.kf.x)\n\n\ndef associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n  """\n  Assigns detections to tracked object (both represented as bounding boxes)\n  Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n  """\n  if len(trackers) == 0:\n    return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n  iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)\n\n  for d, det in enumerate(detections):\n    for t, trk in enumerate(trackers):\n      iou_matrix[d, t] = iou(det, trk)\n  matched_indices = linear_assignment(-iou_matrix)\n\n  unmatched_detections = []\n  for d, det in enumerate(detections):\n    if d not in matched_indices[:, 0]:\n      unmatched_detections.append(d)\n  unmatched_trackers = []\n  for t, trk in enumerate(trackers):\n    if t not in matched_indices[:, 1]:\n      unmatched_trackers.append(t)\n\n  # filter out matched with low IOU\n  matches = []\n  for m in matched_indices:\n    if iou_matrix[m[0], m[1]] < iou_threshold:\n      unmatched_detections.append(m[0])\n      unmatched_trackers.append(m[1])\n    else:\n      matches.append(m.reshape(1, 2))\n  if (len(matches) == 0):\n    matches = np.empty((0, 2), dtype=int)\n  else:\n    matches = np.concatenate(matches, axis=0)\n\n  return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n\n\nclass Sort(object):\n  def __init__(self, max_age=1, min_hits=3):\n    """\n    Sets key parameters for SORT\n    """\n    self.max_age = max_age\n    self.min_hits = min_hits\n    self.trackers = []\n    self.frame_count = 0\n\n  def update(self, dets):\n    """\n    Params:\n      dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n    Requires: this method must be called once for each frame even with empty detections.\n    Returns the a similar array, where the last column is the object ID.\n    NOTE: The number of objects returned may differ from the number of detections provided.\n    """\n    self.frame_count += 1\n    # get predicted locations from existing trackers.\n    trks = np.zeros((len(self.trackers), 5))\n    to_del = []\n    ret = []\n    for t, trk in enumerate(trks):\n      pos = self.trackers[t].predict()[0]\n      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n      if (np.any(np.isnan(pos))):\n        to_del.append(t)\n    trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n    for t in reversed(to_del):\n      self.trackers.pop(t)\n    matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets, trks)\n\n    # update matched trackers with assigned detections\n    for t, trk in enumerate(self.trackers):\n      if (t not in unmatched_trks):\n        d = matched[np.where(matched[:, 1] == t)[0], 0]\n        trk.update(dets[d, :][0])\n\n    # create and initialise new trackers for unmatched detections\n    for i in unmatched_dets:\n      trk = KalmanBoxTracker(dets[i, :])\n      self.trackers.append(trk)\n    i = len(self.trackers)\n    for trk in reversed(self.trackers):\n      d = trk.get_state()[0]\n      if ((trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits)):\n        ret.append(np.concatenate((d, [trk.id + 1])).reshape(1, -1))  # +1 as MOT benchmark requires positive\n      i -= 1\n      # remove dead tracklet\n      if (trk.time_since_update > self.max_age):\n        self.trackers.pop(i)\n    if (len(ret) > 0):\n      return np.concatenate(ret)\n    return np.empty((0, 5))\n\n\ndef parse_args():\n  """Parse input arguments."""\n  parser = argparse.ArgumentParser(description=\'SORT demo\')\n  parser.add_argument(\'--display\', dest=\'display\', help=\'Display online tracker output (slow) [False]\',\n                      action=\'store_true\')\n  args = parser.parse_args()\n  return args\n\n\nif __name__ == \'__main__\':\n  # all train\n  sequences = [\'PETS09-S2L1\', \'TUD-Campus\', \'TUD-Stadtmitte\', \'ETH-Bahnhof\', \'ETH-Sunnyday\', \'ETH-Pedcross2\',\n               \'KITTI-13\', \'KITTI-17\', \'ADL-Rundle-6\', \'ADL-Rundle-8\', \'Venice-2\']\n  args = parse_args()\n  display = args.display\n  phase = \'train\'\n  total_time = 0.0\n  total_frames = 0\n  colours = np.random.rand(32, 3)  # used only for display\n  if (display):\n    if not os.path.exists(\'mot_benchmark\'):\n      print(\n        \'\\n\\tERROR: mot_benchmark link not found!\\n\\n    Create a symbolic link to the MOT benchmark\\n    (https://motchallenge.net/data/2D_MOT_2015/#download). E.g.:\\n\\n    $ ln -s /path/to/MOT2015_challenge/2DMOT2015 mot_benchmark\\n\\n\')\n      exit()\n    plt.ion()\n    fig = plt.figure()\n\n  if not os.path.exists(\'output\'):\n    os.makedirs(\'output\')\n\n  for seq in sequences:\n    mot_tracker = Sort()  # create instance of the SORT tracker\n    seq_dets = np.loadtxt(\'data/%s/det.txt\' % (seq), delimiter=\',\')  # load detections\n    with open(\'output/%s.txt\' % (seq), \'w\') as out_file:\n      print("Processing %s." % (seq))\n      for frame in range(int(seq_dets[:, 0].max())):\n        frame += 1  # detection and frame numbers begin at 1\n        dets = seq_dets[seq_dets[:, 0] == frame, 2:7]\n        dets[:, 2:4] += dets[:, 0:2]  # convert to [x1,y1,w,h] to [x1,y1,x2,y2]\n        total_frames += 1\n\n        if (display):\n          ax1 = fig.add_subplot(111, aspect=\'equal\')\n          fn = \'mot_benchmark/%s/%s/img1/%06d.jpg\' % (phase, seq, frame)\n          im = io.imread(fn)\n          ax1.imshow(im)\n          plt.title(seq + \' Tracked Targets\')\n\n        start_time = time.time()\n        trackers = mot_tracker.update(dets)\n        cycle_time = time.time() - start_time\n        total_time += cycle_time\n\n        for d in trackers:\n          print(\'%d,%d,%.2f,%.2f,%.2f,%.2f,1,-1,-1,-1\' % (frame, d[4], d[0], d[1], d[2] - d[0], d[3] - d[1]),\n                file=out_file)\n          if (display):\n            d = d.astype(np.int32)\n            ax1.add_patch(patches.Rectangle((d[0], d[1]), d[2] - d[0], d[3] - d[1], fill=False, lw=3,\n                                            ec=colours[d[4] % 32, :]))\n            ax1.set_adjustable(\'box-forced\')\n\n        if (display):\n          fig.canvas.flush_events()\n          plt.draw()\n          ax1.cla()\n\n  print("Total Tracking took: %.3f for %d frames or %.1f FPS" % (total_time, total_frames, total_frames / total_time))\n  if (display):\n    print("Note: to get real runtime results run without the option: --display")')
    __stickytape_write_module('lane.py', 'import numpy as np\nimport cv2\nfrom scipy.misc import imresize\nfrom moviepy.editor import VideoFileClip\n#from IPython.display import HTML\nfrom keras.models import load_model\n\n# Class to average lanes with\nclass Lanes():\n    def __init__(self):\n        self.recent_fit = []\n        self.avg_fit = []\n\ndef draw_lane(model, lanes, image):\n    """ Takes in a road image, re-sizes for the model,\n    predicts the lane to be drawn from the model in G color,\n    recreates an RGB image of a lane and merges with the\n    original road image.\n    """\n\n    # Get image ready for feeding into model\n    small_img = imresize(image, (80, 160, 3))\n    small_img = np.array(small_img)\n    small_img = small_img[None,:,:,:]\n\n    # Make prediction with neural network (un-normalize value by multiplying by 255)\n    prediction = model.predict(small_img)[0] * 255\n\n    # Add lane prediction to list for averaging\n    lanes.recent_fit.append(prediction)\n    # Only using last five for average\n    if len(lanes.recent_fit) > 5:\n        lanes.recent_fit = lanes.recent_fit[1:]\n\n    # Calculate average detection\n    lanes.avg_fit = np.mean(np.array([i for i in lanes.recent_fit]), axis = 0)\n\n    # Generate fake R & B color dimensions, stack with G\n    blanks = np.zeros_like(lanes.avg_fit).astype(np.uint8)\n    lane_drawn = np.dstack((blanks, lanes.avg_fit, blanks))\n\n    # Re-size to match the original image\n    lane_image = imresize(lane_drawn, image.shape)\n\n    # Merge the lane drawing onto the original image\n    result = cv2.addWeighted(image, 1, lane_image, 1, 0)\n\n    return result\n')
    import detection_utils
    from keras.models import load_model
    import cv2
    
    def init():
    	model = load_model("./model.py")
    	detection_utils.init(model)
    
    def finish():
    	detection_utils.finalize()
    
    
    def process(img):
    	return detection_utils.detect_image(img)
    
    def draw(img, processed_data):
    	dmodel = load_model("./dmodel.py")
    	total_data = tuple(list(processed_data)+ [dmodel, None])
    	return detection_utils.draw_on_image(img, total_data)
    
    